{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gJ3j1945226"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoUu86p1Or1n"
      },
      "source": [
        "# Prediction-Based Word Vectors\n",
        "\n",
        "more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). Here, we shall explore the embeddings produced by GloVe.\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvpYg_7pODYJ",
        "outputId": "96f8f91c-e205-4401-ab38-56e91973ed44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "import pprint\n",
        "wv_from_bin = api.load(\"glove-wiki-gigaword-200\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFfCOLUsSSuS"
      },
      "source": [
        "### Words with Multiple Meanings\n",
        "Polysemes and homonyms are words that have more than one meaning (see this [wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms ). Find a word with *at least two different meanings* such that the top-10 most similar words (according to cosine similarity) contain related words from *both* meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain **one** of the meanings of the words)?\n",
        "\n",
        "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZAr09U-xSSuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a757f1ab-9fc1-4b40-cde1-0dd8d3185871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: feet\n",
            "Top 10 Similar Words:\n",
            "meters\n",
            "foot\n",
            "above\n",
            "metres\n",
            "tall\n",
            "inches\n",
            "meter\n",
            "around\n",
            "below\n",
            "floor\n"
          ]
        }
      ],
      "source": [
        "### CODE HERE\n",
        "# word = \"date\"\n",
        "# word = \"drop\"\n",
        "word = \"feet\"\n",
        "similar_words = wv_from_bin.most_similar(word)\n",
        "\n",
        "# Print the word and its top 10 similar words\n",
        "print(\"Word:\", word)\n",
        "print(\"Top 10 Similar Words:\")\n",
        "for similar_word, _ in similar_words:\n",
        "    print(similar_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdQ018tjSSuT"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "I've tried multiple words like \"date\" or \"drop\" or other ones. Finally, I decided to set my word as \"feet\".\n",
        "\n",
        "\"feet\" has multiple meanings. one of the is that feet is part of our body. the other one is a measure of distance.\n",
        "\n",
        "**Answer** : many of the attempts with polysemous or homonymic words may not result in success. There are some reasons for it. First one is that different meanings of a polysemous or homonymic word might not have significant semantic overlap. Secondly, The GloVe model is trained on large corpora where certain meanings of a word might be more frequent than others. Polysemous and homonymic words can have different contextual usage, which might not be fully captured by the static word vectors, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfeW-eK9SSuU"
      },
      "source": [
        "### Synonyms & Antonyms\n",
        "\n",
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words $(w_1,w_2,w_3)$ where $w_1$ and $w_2$ are synonyms and $w_1$ and $w_3$ are antonyms, but Cosine Distance $(w_1,w_3) <$ Cosine Distance $(w_1,w_2)$.\n",
        "\n",
        "As an example, $w_1$=\"happy\" is closer to $w_3$=\"sad\" than to $w_2$=\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. Please see the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance)__ for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bwlpPjpHSSuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdec49d-1e37-46f1-e271-370979c7827a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms strong, powerful have cosine distance: 0.3854295015335083\n",
            "Antonyms strong, weak have cosine distance: 0.3302966356277466\n"
          ]
        }
      ],
      "source": [
        "w1 = \"strong\"\n",
        "w2 = \"powerful\"\n",
        "w3 = \"weak\"\n",
        "w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
        "w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
        "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIHjTFMSSuV"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "In this part, we had to choose 3 words. I tried multiple set of words like (Big, Large, Small) or (Fast, Quick, Slow) and finally decided to use the set above. In the first set, distance between antonyms were bigger that the synonyms but second set works correctly and is something same as the one above.\n",
        "\n",
        "**Answer** : There are sone reasons for happening counter-intuitive. one of them is that the words might be synonyms but the context of their usage might be different and when analyzing the words and texts, we will not find out that they are the same meaning. On the other hand, words that are antonyms might have the same usage and are captured in the same context and share the same emotions as their antonyms just in contrast to them.\n",
        "\n",
        "Also the frequency of the words in the corpus is important again.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxIDq26zSSuW"
      },
      "source": [
        "### Analogies with Word Vectors\n",
        "Word vectors have been shown to *sometimes* exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : grandfather :: woman : x\" (read: man is to grandfather as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the `most_similar` function from the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)__. The function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u0pC7H4VSSuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf3e047-f9e9-4360-8a8c-66c9cec22315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('grandmother', 0.7608445286750793),\n",
            " ('granddaughter', 0.7200808525085449),\n",
            " ('daughter', 0.7168302536010742),\n",
            " ('mother', 0.7151536345481873),\n",
            " ('niece', 0.7005682587623596),\n",
            " ('father', 0.6659887433052063),\n",
            " ('aunt', 0.6623408794403076),\n",
            " ('grandson', 0.6618767976760864),\n",
            " ('grandparents', 0.644661009311676),\n",
            " ('wife', 0.6445354223251343)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVv8I9WwSSuZ"
      },
      "source": [
        "Let $m$, $g$, $w$, and $x$ denote the word vectors for `man`, `grandfather`, `woman`, and the answer, respectively. Using **only** vectors $m$, $g$, $w$, and the vector arithmetic operators $+$ and $-$ in your answer, to what expression are we maximizing $x$'s cosine similarity?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would `man` and `woman` lie in the coordinate plane relative to `grandfather` and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUKBqtHSSuZ"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "To solve the analogy \"man : grandfather :: woman : x\", we want to find the word \\( x \\) such that the relationship between ***woman, man,*** and **x** mirrors the relationship between *grandfather* and *man*.\n",
        "\n",
        "Let's denote the word vectors as follows:\n",
        "- ***m*** for \"man\"\n",
        "- ***g*** for \"grandfather\"\n",
        "- ***w*** for \"woman\"\n",
        "- ***x*** for the answer word\n",
        "\n",
        "We want to find this relationship that reflects the relationship between \"man\" and \"grandfather\" and seeks a similar relationship between \"woman\" and \"x\":\n",
        "\n",
        "**g - m ~ x - w**\n",
        "\n",
        "By rearranging the equation, we can get **x** as below:\n",
        "\n",
        "**x ~ g - m + w**  => **x ~ (w - m) + g**\n",
        "\n",
        "This operation effectively transforms the vector for \"woman\" to a vector that represents a word similar to \"grandmother\" in terms of generational relationships.\n",
        "\n",
        "For ploting 2d, we can set the age a y-axis and gender as x-axis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRgMca9SSua"
      },
      "source": [
        "### Finding Analogies\n",
        "a. For the previous example, it's clear that \"grandmother\" completes the analogy. But give an intuitive explanation as to why the `most_similar` function gives us words like \"granddaughter\", \"daughter\", or \"mother?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYQXazQSSua"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "this may happen becauese we are considering vectors for each word in this program. words that are mentioned above might probably have vectors similar to \"woman\" and after getting the result X, while searching for similar words to X, these similarities will be considered and in our result we will have such words.\n",
        "For example: words like \"mother\" and \"daughter\" are both members of the family and we are also looking for a part of the family which is female. So, we might get these words in our result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9aAUXEISSub"
      },
      "source": [
        "b. Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "**Note**: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dhzQJMYYVSjf"
      },
      "outputs": [],
      "source": [
        "x, y, a, b = \"king\", \"queen\", \"man\", \"woman\"\n",
        "assert wv_from_bin.most_similar(positive=[a, y], negative=[x])[0][0] == b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3QlPqAwSSub"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "In this analogy \"king\" is to \"queen\" as \"man\" is to \"woman\". This analogy works because it captures the gender relationship between a male and a female within a monarchy, where the roles of \"king\" and \"queen\" are typically associated with male and female rulers respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwgcEywwSSuc"
      },
      "source": [
        "### Incorrect Analogy\n",
        "a. Below, we expect to see the intended analogy \"hand : glove :: foot : **sock**\", but we see an unexpected result instead. Give a potential reason as to why this particular analogy turned out the way it did?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "m-ykWoJoSSuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6b345b-4108-4d2f-b209-78da0227391c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('45,000-square', 0.4922032654285431),\n",
            " ('15,000-square', 0.4649604558944702),\n",
            " ('10,000-square', 0.4544755816459656),\n",
            " ('6,000-square', 0.44975775480270386),\n",
            " ('3,500-square', 0.444133460521698),\n",
            " ('700-square', 0.44257497787475586),\n",
            " ('50,000-square', 0.4356396794319153),\n",
            " ('3,000-square', 0.43486514687538147),\n",
            " ('30,000-square', 0.4330596923828125),\n",
            " ('footed', 0.43236875534057617)]\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(wv_from_bin.most_similar(positive=['foot', 'glove'], negative=['hand']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn4ruS8MSSud"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "The word \"glove\" can have multiple meanings or associations beside its primary association with hands. If the word \"glove\" in the training data is frequently associated with \"foot\" in contexts related to its other meanings, it could result in not prper answers.\n",
        "\n",
        "also \"hand\" and \"foot\" are both parts of the body, but \"glove\" and \"sock\" are not synonyms despite their similar usage for covering body parts. The model might prioritize semantic similarities between \"foot\" and \"glove\" based on their shared association with the concept of covering body parts, rather than considering the specific semantic relationship implied by the analogy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gHyZt0SSud"
      },
      "source": [
        "b. Find another example of analogy that does *not* hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the **incorrect** value of b according to the word vectors (in the previous example, this would be **'45,000-square'**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D_rlci42XQTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0ce82d-4ac0-49dd-b7e9-8fbd1daf8a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('rottweiler', 0.4928715229034424),\n",
            " ('wag', 0.4848339259624481),\n",
            " ('mcgruff', 0.45675551891326904),\n",
            " ('mixed-breed', 0.4477110505104065),\n",
            " ('bisquick', 0.4014493227005005),\n",
            " ('retriever', 0.39893704652786255),\n",
            " ('hachiko', 0.3969918191432953),\n",
            " ('yelping', 0.3923109769821167),\n",
            " ('sniffed', 0.3909304440021515),\n",
            " ('puppy', 0.3909091055393219)]\n"
          ]
        }
      ],
      "source": [
        "x, y, a, b = \"cat\", \"meow\", \"dog\", \"bark\"\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[a, y], negative=[x]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4x0EHjeSSue"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "The intended analogy \"cat : meow ~ dog : bark\" associates a sound of each animal. However, the word vectors suggest \"rottwiler\" or \"wag\" as the sound of a dog or also we can see \"hachiko\" which is a name for the dog. This is a common association but not specifically the primary vocalization for dogs in the same way that \"bark\" is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvlycXN-SSuf"
      },
      "source": [
        "### Guided Analysis of Bias in Word Vectors\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"profession\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"profession\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XggWA4MhSSuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fcc434-e341-4136-c38a-832a9cae4a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('reputation', 0.5250176787376404),\n",
            " ('professions', 0.5178037881851196),\n",
            " ('skill', 0.49046966433525085),\n",
            " ('skills', 0.49005505442619324),\n",
            " ('ethic', 0.4897659420967102),\n",
            " ('business', 0.4875852167606354),\n",
            " ('respected', 0.485920250415802),\n",
            " ('practice', 0.482104629278183),\n",
            " ('regarded', 0.4778572618961334),\n",
            " ('life', 0.4760662019252777)]\n",
            "\n",
            "[('professions', 0.5957457423210144),\n",
            " ('practitioner', 0.49884122610092163),\n",
            " ('teaching', 0.48292139172554016),\n",
            " ('nursing', 0.48211804032325745),\n",
            " ('vocation', 0.4788965880870819),\n",
            " ('teacher', 0.47160351276397705),\n",
            " ('practicing', 0.46937814354896545),\n",
            " ('educator', 0.46524327993392944),\n",
            " ('physicians', 0.4628995358943939),\n",
            " ('professionals', 0.4601394236087799)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be most dissimilar from.\n",
        "\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4g6KbsYSSuh"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "The list includes terms such as \"teaching\" and \"nursing\"for women.\n",
        "These professions are often associated with women's roles in professions for example their kindness and careness.\n",
        "\n",
        "The list includes terms such as \"business\" and \"practicing\" for men.\n",
        "These terms are often associated with professions in business, law, or skilled trades which shows the traditional look to gender of male.\n",
        "\n",
        "The difference between the lists reflects gender biases in society, which influence professions of every gender. women mostly have jobs like theaching and nursing which men have jobs like business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxJmnS6lSSui"
      },
      "source": [
        "### Independent Analysis of Bias in Word Vectors\n",
        "\n",
        "Use the `most_similar` function to find another pair of analogies that demonstrates some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PZoDheIfSSui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254523d6-1cac-4ff6-d944-b52587776ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('movies', 0.6726292371749878),\n",
            " ('film', 0.6012022495269775),\n",
            " ('films', 0.5982237458229065),\n",
            " ('story', 0.5867626070976257),\n",
            " ('?', 0.566396951675415),\n",
            " ('comedy', 0.5641261339187622),\n",
            " ('seems', 0.5593376755714417),\n",
            " ('_', 0.5523263216018677),\n",
            " ('disney', 0.552110493183136),\n",
            " ('something', 0.5514143109321594)]\n",
            "\n",
            "[('film', 0.6618216633796692),\n",
            " ('hindi', 0.6286905407905579),\n",
            " ('films', 0.6040933728218079),\n",
            " ('malayalam', 0.6018020510673523),\n",
            " ('movies', 0.5716251134872437),\n",
            " ('starring', 0.5467848777770996),\n",
            " ('made-for-television', 0.5456230044364929),\n",
            " ('bachchan', 0.5428180694580078),\n",
            " ('soundtrack', 0.5388950109481812),\n",
            " ('remake', 0.5343798398971558)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "A = \"hollywood\"\n",
        "B = \"bollywood\"\n",
        "word = \"movie\"\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[A, word], negative=[B]))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[B, word], negative=[A]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGOlmtJoSSuj"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "When searching for words similar to \"hollywood\" and \"movie\" but dissimilar to \"bollywood\", we find terms like \"film\", \"cinema\", and \"desney\" which is an american company. This reflects a bias where the term \"movie\" or \"film\" is closely associated with both Hollywood and Bollywood.\n",
        "\n",
        "Conversely, when searching for words similar to \"bollywood\" and \"movie\" but dissimilar to \"hollywood\", we find terms like \"film\", \"hindi\", and \"malayam\" highly ranked.\n",
        "\n",
        "This bias is likely influenced by the similarities between Hollywood and bollywood that results in similar words but there are also lots of different words for each one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK2XVWzmSSuk"
      },
      "source": [
        "### Thinking About Bias\n",
        "\n",
        "a. Give one explanation of how bias gets into the word vectors. Briefly describe a real-world example that demonstrates this source of bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19pM85fCSSuk"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "One explanation is that the biases exist in the corpus used to train the models. Word embeddings are trained on large corpora of text data, which often reflect some biases. As a result, these biases can become encoded into the semantic relationships captured by the word vectors.\n",
        "\n",
        "A real-world example of how bias gets into word vectors is through historical biases present in literature and other written texts that serve as the training data for these models.\n",
        "\n",
        "For instance, consider word embeddings trained on a corpus of historical texts or literature from a particular time period. Such texts may reflect the biases in society during that era. Words and phrases used in these texts may maintain unequal power dynamics based on factors such as race, gender, religion, and economic status.\n",
        "\n",
        "An example of this source of bias can be observed in word embeddings trained on literature from the early 20th century. During this period, racial segregation and discrimination were widespread in many parts of the world. Texts from this era may contain derogatory language, stereotypes, and negative portrayals of marginalized groups, reinforcing societal biases and prejudices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILYqJZ7ASSul"
      },
      "source": [
        "b. What is one method you can use to mitigate bias exhibited by word vectors?  Briefly describe a real-world example that demonstrates this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnJaAB7mSSul"
      },
      "source": [
        "\n",
        "### SOLUTION\n",
        "\n",
        "One method to mitigate bias exhibited by word vectors is debiasing techniques, which aim to remove or reduce biased associations present in the word embeddings. One common approach is to identify and neutralize biased gender, race, or other demographic associations by modifying the vector space to ensure that certain sensitive attributes are not predictive of word embeddings.\n",
        "\n",
        "For example, consider the debiasing technique proposed by Bolukbasi et al. (2016), which uses a gender subspace projection to neutralize gender bias in word embeddings. This method involves identifying a subspace in the vector space that captures gender-specific associations and then projecting word vectors onto a gender-neutral subspace.\n",
        "\n",
        "A real-world example of this method in action can be seen in efforts to mitigate gender bias in word embeddings used for natural language processing tasks. Researchers have applied debiasing techniques to word embeddings trained on large text corpora to reduce gender stereotypes and ensure fair and unbiased representations of gender-related concepts. By neutralizing gender associations in word vectors, these debiasing methods help improve the fairness and equity of machine learning models and applications, such as hiring algorithms, language translation systems, and sentiment analysis tools."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_eGFD7kayHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sFfCOLUsSSuS",
        "VfeW-eK9SSuU",
        "ZxIDq26zSSuW",
        "WgYQXazQSSua"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}